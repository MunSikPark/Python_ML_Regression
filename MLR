# 1. Library & Data Import
%matplotlib inline

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('C:\\Users\\HP\\Desktop\\Lecture\\Korea_Univ\\Multi_Variative_data\\02_Multiple_Linear_Regression\\R_Exercise_MLR\\ToyotaCorolla.csv')

#2. EDA: Exploratory Data Analysis_탐구 데이터 분석
# 그래프 배경 설정
sns.set_style('darkgrid')

#2-1. 데이터셋 기본 정보 파악
# shape (dimension)
df.shape
# 결측치
df.isnull().sum()
# data type
df.info()
# numerical variable
df.describe()
# categorical variable
num_town = df['TOWN'].unique()
print(len(num_town))
num_town

# 2-2. 종속 변수(목표 변수) 탐색
# >> Target Variable: ‘CMEDV’(주택 가격) 탐색
# 기초 통계량
df['CMEDV'].describe()
# 분포
df['CMEDV'].hist(bins=50)
# Pandas Function (pandas.DataFrame.boxplot)
# Matplotlib Function (matplotlib.pyplot.boxplot)
# boxplot - Pandas
df.boxplot(column=['CMEDV'])
plt.show()
# boxplot - matplotlib
plt.boxplot(df['CMEDV'])
plt.show()

# 2-3. 설명 변수 탐색
# >> 설명 변수의 분포 탐색
# numerical features (except "LON" & "LAT")
numerical_columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
fig = plt.figure(figsize = (16, 20))
ax = fig.gca()  # Axes 생성
df[numerical_columns].hist(ax=ax)
plt.show()

# 2-4. 설명변수와 종속변수 간의 관계 탐색
# Person 상관계수
cols = ['CMEDV', 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
corr = df[cols].corr(method = 'pearson')
corr

#상관계수를 좀 더 직관적인 Heatmap으로 표현
# heatmap (seaborn)
fig = plt.figure(figsize = (16, 12))
ax = fig.gca()
sns.set(font_scale = 1.5)  # heatmap 안의 font-size 설정
heatmap = sns.heatmap(corr.values, annot = True, fmt='.2f', annot_kws={'size':15},
                      yticklabels = cols, xticklabels = cols, ax=ax, cmap = "RdYlBu")
plt.tight_layout()
plt.show()

# target variable **“CMEDV - 주택 가격”**과 다른 변수간의 상관관계를 살펴보면, **“CMEDV - 주택 가격”**은 “RM - 자택당 평균 방 갯수”(0.7) 및 **“LSTAT - 빈곤층의 비율”(-0.74)**과 강한 상관관계를 보이고 있음.
# 이 두 변수와의 관계를 좀 더 자세히 살펴볼게요.

# 주택 가격 ( “CMEDV” ) ~ 방 갯수 ( “RM” )
# scatter plot
sns.scatterplot(data=df, x='RM', y='CMEDV', markers='o', color='blue', alpha=0.6)
plt.title('Scatter Plot')
plt.show()

# 주택 가격(“CMEDV”) ~ 빈곤층의 비율(“LSTAT”)
# scatter plot
sns.scatterplot(data=df, x='LSTAT', y='CMEDV', markers='o', color='blue', alpha=0.6)
plt.title('Scatter Plot')
plt.show()

# >> 도시별 차이 탐색
# 데이터를 살펴보면 여러 지역이 같은 도시에 속한 경우가 있습니다. 변수 중에서도 도시 단위로 측정되는 변수가 많고요. 따라서 우리는 자연스럽게 도시 간의 차이를 궁금하게 됩니다.
# 먼저 각 도시의 데이터 갯수부터 살펴볼게요.
# 도시별 데이터 갯수
df['TOWN'].value_counts()

# 도시별 데이터 갯수 (bar plot)
df['TOWN'].value_counts().hist(bins=50)

# 이제 각 도시의 주택 가격 분포를 box plot을 통해 표현 해보겠습니다.
# 도시별 주택 가격 특징 (boxplot 이용)
fig = plt.figure(figsize = (12, 20))
sns.boxplot(x='CMEDV', y='TOWN', data=df)

# 도시별 범죄율 특징
# fig = plt.figure(figsize = (12, 20))
# sns.boxplot(x='CRIM', y='TOWN', data=df)

#3. 주택 가격 예측 모델링: 회귀 분석
# 문자형 변수인 "TOWN"와 범주형 변수인 “CHAS” (Dummy variable)를 제외하여 모든 수치형 변수에 대해서 표준화를 진행합니다.

# feature standardization  (numerical_columns except dummy var.-"CHAS")
scaler = StandardScaler()  # 평균 0, 표준편차 1
scale_columns = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
df[scale_columns] = scaler.fit_transform(df[scale_columns])

# features for linear regression model
df[numerical_columns].head()

from sklearn.model_selection import train_test_split

# split dataset into training & test
X = df[numerical_columns]
y = df['CMEDV']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# >> 다중공선성
# 회귀 분석에서 하나의 feature(예측 변수)가 다른 feature와의 상관 관계가 높으면(즉, 다중공선성이 존재하면), 회귀 분석 시 부정적인 영향을 미칠 수 있기 때문에, 모델링 하기 전에 먼저 다중공선성의 존재 여부를 확인해야합니다.
# 보통 다중공선성을 판단할 때 VIF값을 확인합니다. 일반적으로, VIF > 10인 feature들은 다른 변수와의 상관관계가 높아, 다중공선성이 존재하는 것으로 판단합니다.

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()
vif['features'] = X_train.columns
vif["VIF Factor"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif.round(1)

# 3-2. 회귀 모델링
# 먼저 Training set에서 선형 회귀 예측 모델을 학습합니다.
# 그 다음 도출된 모델을 Test set에 적용해 주택 가격(“CMEDV”)을 예측합니다. 이 결과는 다중에 실제 “CMEDV” 값과 비교하여 모델의 예측 성능을 평가하는 데 활용하게 됩니다

from sklearn import linear_model

# fit regression model in training set
lr = linear_model.LinearRegression()
model = lr.fit(X_train, y_train)

# predict in test set
pred_test = lr.predict(X_test)

# 3-3. 모델 해석
# >> coefficients 확인하기
# print coef
print(lr.coef_)
# "feature - coefficients" DataFrame 만들기
coefs = pd.DataFrame(zip(df[numerical_columns].columns, lr.coef_), columns = ['feature', 'coefficients'])
coefs
# 크기 순서로 나열
coefs_new = coefs.reindex(coefs.coefficients.abs().sort_values(ascending=False).index)
coefs_new

## coefficients 시각화

# figure size
plt.figure(figsize = (8, 8))

# bar plot
plt.barh(coefs_new['feature'], coefs_new['coefficients'])
plt.title('"feature - coefficient" Graph')
plt.xlabel('coefficients')
plt.ylabel('features')
plt.show()

# >> feature 유의성 검정
import statsmodels.api as sm

X_train2 = sm.add_constant(X_train)
model2 = sm.OLS(y_train, X_train2).fit()
model2.summary()

# Warnings:
# [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

# >> 주택 가격 영향 요소

# 위 결과를 종합해 보면, 주택 가격의 영향 요소에 관하여 다음과 같은 결론들을 도출할 수 있습니다:

# "INDUS"(상업적 비즈니스에 활용되지 않는 농지 면적)과 “AGE”(1940년 이전에 건설된 비율)은 유의하지 않습니다. (p value > 0.05)

# "ZN"(25,000 제곱 피트(sq.ft) 이상의 주택지 비율),
# "CHAS"(Charles 강과 접하고 있는지 여부),
# "RM"(자택당 평균 방 갯수),
# "RAD"(소속 도시가 Radial 고속도로와의 접근성 지수),
# "B"(흑인 지수)는
# 주택 가격에 Positive한 영향을 미칩니다.
# 즉, 다른 변수의 값이 고정했을 때, 해당 변수의 값이 클수록 주택의 가격이 높을 것입니다.

# "CRIM"(지역 범죄율),
# "NOX"(산화질소 농도),
# "DIS"(보스턴 고용 센터와의 거리),
# "TAX"(재산세),
# "PTRATIO"(학생-교사 비율),
# "LSTAT"(빈곤층 비율)은
# 주택 가격에 Negative한 영향을 미칩니다.
# 즉, 다른 변수의 값이 고정했을 때, 해당 변수의 값이 작을수록 주택의 가격이 높을 것입니다.

# 3-4. 모델 예측 결과 및 성능 평가
# >> 예측 결과 시각화

# 학습한 모델을 Test set에 적용하여 y값(“CMEDV”)을 예측합니다.
# 예측 결과를 확인하기 위해 실제값과 예측값을 한 plot에 출력해 시각화해보겠습니다.

# 예측 결과 시각화 (test set)
df = pd.DataFrame({'actual': y_test, 'prediction': pred_test})
df = df.sort_values(by='actual').reset_index(drop=True)

plt.figure(figsize=(12, 9))
plt.scatter(df.index, df['prediction'], marker='x', color='r')
plt.scatter(df.index, df['actual'], alpha=0.7, marker='o', color='black')
plt.title("Prediction Result in Test Set", fontsize=20)
plt.legend(['prediction', 'actual'], fontsize=12)
plt.show()

# R square
print(model.score(X_train, y_train))  # training set
print(model.score(X_test, y_test))  # test set

# RMSE
from sklearn.metrics import mean_squared_error
from math import sqrt

# training set
pred_train = lr.predict(X_train)
print(sqrt(mean_squared_error(y_train, pred_train)))

# test set
print(sqrt(mean_squared_error(y_test, pred_test)))
